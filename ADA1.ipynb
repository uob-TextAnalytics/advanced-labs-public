{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ba36903-ee71-4a62-9bb8-8e0270fdae64",
   "metadata": {},
   "source": [
    "# Advanced Text Analytics Lab 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de31fe1-70a5-4357-b9d9-be40038c556c",
   "metadata": {},
   "source": [
    "This notebook is the first of two lab notebooks that you will submit as part of your assessment for the Advanced Data Analytics unit. \n",
    "\n",
    "This notebook is contains three sections:\n",
    "1. **Word embeddings:** This will introduce you to loading and training word embeddings using the Gensim library.\n",
    "2. **Introducing neural text classifiers:** Here we show you how to construct a neural network text classifier for sentiment analysis using Pytorch. \n",
    "3. **Improving neural text classifiers:** This section gives you a chance to improve the classifier from the previous section by applying what we have learned in the lectures.\n",
    "\n",
    "## Learning Outcomes\n",
    "\n",
    "These sections will contain tutorial-like instructions, as you have seen in previous text analytics labs. On completing these sections, the intended learning outcomes are that you will be able to...\n",
    "1. Load pretrained word embeddings models.\n",
    "1. Learn word embeddings from an unlabelled dataset.\n",
    "1. Recognise the steps required to train and test a neural text classifier with Pytorch\n",
    "1. Adapt the architecture of a neural text classifier.\n",
    "\n",
    "## Getting Started -- Python Packages\n",
    "\n",
    "Please see the README.md file for instructions on setting up your Python environment. The readme will instruct you to install the required packages, in addition to those used for Introduction to Data Analytics:\n",
    "\n",
    " * pytorch=1.9.0\n",
    " * scipy=1.8.0\n",
    " * transformers=2.1.1\n",
    "\n",
    "## Your Tasks\n",
    "\n",
    "Inside each of these sections there are several **'To-do's**, which you must complete for your summative assessment. Your marks will be based on your answers to these to-dos. Please make sure to:\n",
    "1. Include the output of your code in the saved notebook. Plots and printed output should be visible without re-running the code. \n",
    "1. Include all code needed to generate your answers.\n",
    "1. Provide sufficient comments to understand how your method works.\n",
    "1. Write text in a cell in markdown format where a written answer is required. You can convert a cell to markdown format by pressing Escape-M. \n",
    "\n",
    "There are also some unmarked 'to-do's that are part of the tutorial to help you learn how to implement and use the methods studied here. These do not contribute to your final marks.\n",
    "\n",
    "## Good Academic Practice\n",
    "\n",
    "Please follow [the guidance on academic integrity provided by the university](http://www.bristol.ac.uk/students/support/academic-advice/academic-integrity/).\n",
    "You are required to write your own answers -- do not share your notebooks or copy someone else's writing. Do not copy text or long blocks of code directly into the notebook from online sources -- always rewrite in your own way. Breaking the rules can lead to strong penalties. \n",
    "\n",
    "## Marking Criteria\n",
    "\n",
    "1. The coursework (both notebooks) is worth 30% of the unit in total. \n",
    "1. There is a total of 100 marks available for both lab notebooks. \n",
    "1. This notebook is worth 50 of those marks.\n",
    "1. The number of marks for each to-do out of 100 is shown alongside each to-do.\n",
    "1. For to-dos that require you to write code, a good solution would meet the following criteria (in order of importance):\n",
    "   1. Solves the task or answers the question asked in the to-do. This means, if the code cells in the notebook are executed in order, we will get the output shown in your notebook.\n",
    "   1. The code is easy to follow and does not contain unnecessary steps.\n",
    "   1. The comments show that you understand how your solution works.\n",
    "   1. A very good answer will also provide code that is computationally efficient but easy to read.\n",
    "1. You can use any suitable publicly available libraries. Unless the task explicitly asks you to implement something from scratch, there is no penalty for using libraries to implement some steps.\n",
    "\n",
    "## Support\n",
    "\n",
    "The main source of support will be during the remaining lab sessions (Fridays 3-6pm) for this unit. \n",
    "\n",
    "The TAs and lecturer will help you with questions about the lectures, the code provided for you in this notebook, and general questions about the topics we cover. For the marked 'to-dos', they can only answer clarifying questions about what you have to do. \n",
    "\n",
    "Office hours: You can book office hours with Edwin on Mondays 3pm-5pm by sending him an email (edwin.simpson@bristol.ac.uk). If those times are not possible for you, please contact him by email to request an alternative. \n",
    "\n",
    "## Deadline\n",
    "\n",
    "The notebook must be submitted along with the second notebook on Blackboard before **Wednesday 24th May at 13.00**. \n",
    "\n",
    "## Submission\n",
    "\n",
    "You will need to zip up this notebook and the next notebook into a single .zip file, which you will submit to Blackboard through the 'assessment, submission and feedback' link on the left sidebar. \n",
    "\n",
    "Please name your files like this:\n",
    "   * Name this notebook 'ADA1_\\<student_number\\>.ipynb'\n",
    "   * Name the zip file '\\<student_number\\>.zip'\n",
    "   * Please don't include your name as we want to mark anonymously to ensure fairness. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c549d0b9-84d1-4d35-80b1-6341a3ea7ce6",
   "metadata": {},
   "source": [
    "# 1. Word Embeddings (max. 12 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1a8852-b72a-48dd-a266-25a4bb774f6a",
   "metadata": {},
   "source": [
    "In this section we will use both sparse vectors and dense word2vec embeddings to obtain\n",
    "vector representations of words and documents. \n",
    "\n",
    "First, we will load the `tweet eval` sentiment dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5519c4b9-1cac-4e8a-9e7e-a8a40b389bdf",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e599bf09-ed1a-4e40-a551-2e3a889a3487",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cache_dir = \"./data_cache\"\n",
    "\n",
    "train_dataset = load_dataset(\n",
    "    \"tweet_eval\",\n",
    "    name=\"sentiment\",\n",
    "    split=\"train\",\n",
    "    ignore_verifications=True,\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "\n",
    "print(f\"Training dataset with {len(train_dataset)} instances loaded\")\n",
    "\n",
    "\n",
    "dev_dataset = load_dataset(\n",
    "    \"tweet_eval\",\n",
    "    name=\"sentiment\",\n",
    "    split=\"validation\",\n",
    "    ignore_verifications=True,\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "\n",
    "print(f\"Development/validation dataset with {len(dev_dataset)} instances loaded\")\n",
    "\n",
    "\n",
    "test_dataset = load_dataset(\n",
    "    \"tweet_eval\",\n",
    "    name=\"sentiment\",\n",
    "    split=\"test\",\n",
    "    ignore_verifications=True,\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "\n",
    "print(f\"Test dataset with {len(test_dataset)} instances loaded\")\n",
    "\n",
    "# Put the data into lists ready for the next steps...\n",
    "train_texts = []\n",
    "train_labels = []\n",
    "for i in tqdm(range(len(train_dataset))):\n",
    "    train_texts.append(train_dataset[i]['text'])\n",
    "    train_labels.append(train_dataset[i]['label'])\n",
    "            \n",
    "# HINT: A count vectorizer object may be useful in later steps\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(train_texts)\n",
    "\n",
    "# Get the vocabulary\n",
    "vocab = vectorizer.vocabulary_\n",
    "vocab_size = len(vocab)\n",
    "print(f'The vocabulary has {vocab_size} words')\n",
    "\n",
    "# invert the vocabulary dictionary so we can look up word types given an index\n",
    "keys = vocab.values()\n",
    "values = vocab.keys()\n",
    "vocab_inverted = dict(zip(keys, values))\n",
    "\n",
    "print(f'Index of \"love\" is {vocab[\"love\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a63c3c-d24c-4810-9c34-4532b845e310",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.1. Term-Document Matrix\n",
    "\n",
    "First we are going to obtain sparse word vectors from a term-document matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd30abb-0951-46ee-81a6-b9217fba6b90",
   "metadata": {},
   "source": [
    "**TO-DO 1a:** Use CountVectorizer to obtain a term-document matrix for the training set. Then, write a function that takes a word as an argument and returns its term vector from the term-document matrix you computed. Get the term vector for the word 'love'. **(4 marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a52da7-89a5-41d6-bfab-83b3231d6949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE YOUR ANSWER HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a286c6e1-f3e9-4c40-b968-0c24eb8dbaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of words for comparison with 'love' in the next to-do.\n",
    "comparison_words = ['2wee', '4your', 'follow', 'goodnight', 'liam', 'lol', 'okay', 'sorry',\n",
    " 'wish', 'yea', 'affair', 'agree', 'all', 'alliums', 'alliumsvancouver', 'always',\n",
    " 'amazing', 'and', 'appreciate', 'ask', 'babe', 'baby', 'bandit76044', 'barat',\n",
    " 'beautiful', 'birthday', 'boy', 'bro', 'btw', 'but', 'commando', 'content',\n",
    " 'dear', 'dm', 'dream', 'dreams', 'enjoy', 'enjoyed', 'everything', 'fam',\n",
    " 'followers', 'for', 'forever', 'forget', 'friend', 'friends', 'gabrielle',\n",
    " 'girl', 'god', 'good', 'guys', 'hahaha', 'happy', 'hate', 'hello', 'hey',\n",
    " 'homework', 'hope', 'in', 'invite', 'is', 'isabel', 'it', 'jonny', 'kiss', 'know',\n",
    " 'krishna', 'ladies', 'let', 'life', 'like', 'lil', 'little', 'love', 'loved',\n",
    " 'loves', 'loving', 'lucky', 'luv', 'ma', 'may', 'me', 'mean', 'meet', 'met', 'miss',\n",
    " 'much', 'my', 'notice', 'nsfanfic', 'nuffsaid', 'nya', 'of', 'on', 'one',\n",
    " 'ontario', 'perfect', 'prefer', 'queen', 'rails', 'rather', 'recommend',\n",
    " 'remember', 'see', 'share', 'sing', 'smile', 'so', 'suggest', 'sunat', 'sweet',\n",
    " 'tag', 'tail', 'tebaklagu', 'thank', 'thanks', 'the', 'this', 'thsoul', 'to',\n",
    " 'tomorrow', 'too', 'true', 'unreservedly', 'user', 'want', 'weed', 'what', 'wish',\n",
    " 'wishes', 'with', 'women_of_christ', 'would', 'wow', 'xxxxxx', 'yay', 'yes',\n",
    " 'you', 'your', 'zorro',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb324d8-a92a-4fd7-bd7f-f3b0f0958e6b",
   "metadata": {},
   "source": [
    "**TO-DO 1b:** Write a function that computes the similarity between two different term vectors. For this to-do, do not simply call a library function that implements a similarity function, implement the calculation yourself. Use the function to find the five most similar terms to \"love\" from the list of `comparison_words` given above. **(6 marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2cca67-685e-4760-92be-eea50ed87769",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# WRITE YOUR OWN CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9654fde-cbe7-4020-99f7-49d97b4eacfa",
   "metadata": {},
   "source": [
    "## 1.2 Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e772ce7-853e-478d-9d0a-3f591100e88d",
   "metadata": {},
   "source": [
    "Now, we will use Gensim to train a word2vec model. The code below tokenizes the training texts, then runs word2vec (the skipgram model) to learn a set of embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5d13e1-683f-41f6-bf24-7c036aef59fa",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "from gensim.utils import tokenize\n",
    "\n",
    "tokenized_texts = [list(tokenize(text)) for text in train_texts]\n",
    "emb_model = word2vec.Word2Vec(tokenized_texts, sg=1, min_count=1, window=3, vector_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989624ac-dc98-461f-a5ad-c28cf27ca085",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the word vector for 'love'\n",
    "love_embedding = emb_model.wv['love']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccd66ea-cbbe-4c1a-a467-5a784facf33b",
   "metadata": {},
   "source": [
    "**TODO 1c:** Find the five most similar words to 'love' according to your word2vec model. You can use the Gensim function `similar_by_word` to do this. How does the Word2Vec top 5 differ from the top 5 comparison words found using the term-document matrix? **(2 marks)**\n",
    "\n",
    "WRITE YOUR ANSWER HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc37a093-db74-4e10-9224-0740381b30fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE YOUR OWN CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb28de29-ee93-47e9-ba62-de7771d7203d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Above, we trained our own model using the skipgram method. We can also download a pretrained model that has previously been trained on a large corpus. There is a list of models available [here](https://radimrehurek.com/gensim/models/word2vec.html#pretrained-models). Let's try out GLoVe embeddings. GLoVe is an alternative to the skipgram model. This model was trained on a corpus of tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693126a6-519a-44c1-b800-773fc95d38df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "\n",
    "glove_wv = gensim.downloader.load('glove-twitter-25')\n",
    "\n",
    "# show the vector for Hamlet:\n",
    "print(glove_wv['love'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcbeb21-549d-4315-bce1-e785cda13fd2",
   "metadata": {},
   "source": [
    "TODO 1d: Find the most similar five words to 'happy' according to the GloVe Twitter model. (this task is unmarked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6893470a-08b7-4f44-b5f0-8faddb91ae85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE YOUR OWN CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc39168a-7b01-4171-beb2-8a4bf035fd71",
   "metadata": {},
   "source": [
    "Notice again that a different set of words are favoured than with word2vec or term-document vectors, and consider how this might result from pretraining the embeddings on Twitter data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421d24e4-d90d-4f50-a167-5f4876076355",
   "metadata": {},
   "source": [
    "# 2. Introducing Neural Text Classifiers (max. 16 marks)\n",
    "\n",
    "This section shows you how to implement a neural network classifier using Pytorch and leads you through the steps required to process text sequences.\n",
    "\n",
    "There are several big advantages to building a text classifier using a neural network:\n",
    "   * It can model nonlinear functions, so can handle much more complex relationships between features and class labels.\n",
    "   * It performs representation learning: the hidden layers learn how to extract features from low-level data.\n",
    "   * It can process sequences of tokens -- we don't have to think in terms of a single feature vector representing a document as we did for logistic regression.\n",
    "  \n",
    "The downsides are:\n",
    "   * Much more expensive to train and test.\n",
    "   * It can overfit very badly to small datasets.\n",
    "   * The features learned by the hidden layers can be hard to interpret, which can make it hard to predict the model's behaviour, e.g., what sort of cases it may fail on.\n",
    "   \n",
    "Let's start by building a neural network text classifier that takes a sequence of tokens as input, and predicts a class label. For simplicity, it will use a single fully connected feedforward layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca4f88e-bd86-4917-8e9d-26ace635873e",
   "metadata": {},
   "source": [
    "\n",
    "We are going to construct the neural network in this form:\n",
    "\n",
    "<img src=\"neural_text_classifier_smaller.png\" alt=\"Neural text classifier diagram\" width=\"600px\"/>\n",
    "\n",
    "The first step -- as always -- is to get our data into the right format. We start from a set of tokenised documents (in this case, tweets), where each document is represented as a sequence of text tokens. The neural network cannot process the tokens as strings, so we need to convert each token to a numerical input value. The input value for each token is used to look up the corresponding embedding in the embedding layer. For PyTorch, it's not necessary to create one-hot vectors for each token, as library just uses the indexes of the words in the vocabulary to look up the corresponding word embedding. \n",
    "\n",
    "So, let's now map the tokens to their IDs -- their indexes in the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a71101-e504-42a8-9862-74796af52cd0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# tokenize training set and convert to input IDs.\n",
    "def encode_text(sample):\n",
    "    tokens = tokenize(sample['text'])  # Tokenize one document\n",
    "    \n",
    "    input_ids = []\n",
    "    for token in tokens:\n",
    "        if str.lower(token) in vocab:  # Skip words from the dev/test set that are not in the vocabulary.\n",
    "            input_ids.append(vocab[str.lower(token)]+1) # +1 is needed because we reserve 0 as a special character\n",
    "            \n",
    "    sample['input_ids'] = input_ids \n",
    "    return sample\n",
    "\n",
    "# The map method of the dataset object takes a function as its argument, \n",
    "# and applies that function to each document in the dataset.\n",
    "train_dataset = train_dataset.map(encode_text)\n",
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ce8d8c-ce83-49b7-9b88-b84233be7dbf",
   "metadata": {},
   "source": [
    "Our neural network's input layer has a fixed size, so we need to make all of our documents have the same number of tokens. Let's plot a histogram to understand the length distribution of the texts in the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832da949-3fec-4d26-936e-23c943546b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rv_l = [len(doc) for doc in train_dataset['input_ids']]\n",
    "print('Mean of the document length: {}'.format(np.mean(rv_l)))\n",
    "print('Median of the document length: {}'.format(np.median(rv_l)))\n",
    "print('Maximum document length: {}'.format(np.max(rv_l)))\n",
    "\n",
    "plt.hist(rv_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da97d92c-7002-4b63-9011-8ed21ea52e95",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We now neeed to choose a fixed sequence length, then *pad* the documents that are shorter than this maximum by adding a special token to the start of the sequence. The special pad token has an input value of 0. Any documents that exceed the length will be truncated.\n",
    "\n",
    "**TO-DO 2a:** Complete the padding code below to insert 0s at the start of any sequences that are too short, and to truncate any sequences that are too long. **(3 marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8488dfa-61b8-438e-92ee-7f9b84b27a68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sequence_length = 40  # truncate all docs longer than this. Pad all docs shorter than this.\n",
    "\n",
    "def pad_text(sample):\n",
    "    ###WRITE YOUR OWN CODE HERE\n",
    "\n",
    "    \n",
    "    ##########\n",
    "    \n",
    "    return sample\n",
    "\n",
    "# The map method will call pad_text for every document in the dataset\n",
    "train_dataset = train_dataset.map(pad_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fb33a1-ca74-4a95-bbd5-2b9e50167733",
   "metadata": {},
   "source": [
    "We now have our data in almost the right format! To train a model using PyTorch, we are going to wrap our dataset in a [DataLoader object](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader). This allows the training process to select random subsets of the dataset -- mini-batches -- which it will use for learning with mini-batch stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab3c226-402b-4eb5-8f9a-fe216b4c5da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# convert from the Huggingface format to a TensorDataset so we can use the mini-batch sampling functionality\n",
    "def convert_to_data_loader(dataset, num_classes):\n",
    "    # convert from list to tensor\n",
    "    input_tensor = torch.from_numpy(np.array(dataset['input_ids']))\n",
    "    label_tensor = torch.from_numpy(np.array(dataset['label'])).long()\n",
    "    tensor_dataset = TensorDataset(input_tensor, label_tensor)\n",
    "    loader = DataLoader(tensor_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return loader\n",
    "\n",
    "num_classes = len(np.unique(train_labels))   # number of possible labels in the sentiment analysis task\n",
    "\n",
    "train_loader = convert_to_data_loader(train_dataset, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a5825c-4a0e-4fc8-9de6-504bc7227377",
   "metadata": {},
   "source": [
    "Let's process the development and test set as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78aa6ddc-b936-431c-865e-dc55eaf3086c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dataset = dev_dataset.map(encode_text)\n",
    "dev_dataset = dev_dataset.map(pad_text)\n",
    "dev_loader = convert_to_data_loader(dev_dataset, num_classes)\n",
    "\n",
    "test_dataset = test_dataset.map(encode_text)\n",
    "test_dataset = test_dataset.map(pad_text)\n",
    "test_loader = convert_to_data_loader(test_dataset, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7b857f-5d57-44fd-a1b4-e1b31a009c72",
   "metadata": {},
   "source": [
    "As shown in the diagram above, we will build a NN with three different layers for sentiment classification.\n",
    "\n",
    "### Embedding layer\n",
    "In the embedding layer, the network will create its own embeddings for the index with a given embedding dimension.\n",
    "The module `nn.Embedding()` creates a simple lookup table that stores embeddings of a fixed dictionary and size.\n",
    "This module is often used to store word embeddings and retrieve them using indices.\n",
    "The module's input is a list of indices, and the output is the corresponding word embeddings.\n",
    "\n",
    "[Documentation for Embedding Class](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)\n",
    "\n",
    "### Fully-connected layer\n",
    "Fully-connected layers in a neural network are those layers where all the inputs from the previous layer are connected to every unit of the fully-connected layer. Here we will use fully-connected layers for the hidden layer and output layer. In Pytorch this kind of layer is implemented by the 'Linear' class:\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.Linear.html\n",
    "\n",
    "## Activation functions\n",
    "In Pytorch, the activation function is not included in the Linear class (or other kinds of neural network layer), so we need to explicitly connect each layer to an activation function.\n",
    "In Pytorch, we construct a neural network by connecting up the output of each component to the input of the next, thereby creating a computation graph.\n",
    "To complete the hidden layer, we connect the ouput of the linear layer to a ReLU activation function, thereby creating a nonlinear function.\n",
    "\n",
    "The cell below defines a class for our neural text classifier. The constructor creates each of the layers and the activations. The dimensions of each layer need to be correct so that the output of one layer can be passed as input to the next, but the code is not yet complete.\n",
    "\n",
    "Below the constructor is the forward method. This is called in the 'forward pass' to map the neural network's inputs to its outputs. In PyTorch, we pass data through each layer of the model, connecting them together, then returning the output of the final layer.\n",
    "\n",
    "**TO-DO 2b** Complete the constructor and the forward method below for a NN with three layers. The places where you need to add code are marked in the cell below. Refer to the Pytorch documentation for additional help.  **(2 marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38a13cf-4b4d-4ac2-868c-8ad5c1b72b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class FFTextClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, sequence_length, embedding_size, hidden_size, num_classes):\n",
    "        super(FFTextClassifier, self).__init__()\n",
    "\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        # Here we just need to construct the components of our network. We don't need to connect them together yet.\n",
    "        self.embedding_layer = nn.Embedding(vocab_size, embedding_size) # embedding layer\n",
    "        \n",
    "        ### COMPLETE THE CODE HERE: WRITE IN THE MISSING ARGUMENTS SPECIFYING THE DIMENSIONS OF EACH LAYER\n",
    "        self.hidden_layer = nn.Linear(      ) # Fully connected hidden layer\n",
    "        self.activation = nn.ReLU(      ) # Hidden layer\n",
    "        ##########\n",
    "        \n",
    "        self.output_layer = nn.Linear(hidden_size, num_classes) # Fully connected output layer\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward (self, input_words):\n",
    "        # Input dimensions are:  (batch_size, seq_length)\n",
    "        embedded_words = self.embedding_layer(input_words)  # (batch_size, seq_length, embedding_size)\n",
    "\n",
    "        # flatten the sequence of embedding vectors for each document into a single vector.\n",
    "        embedded_words = embedded_words.reshape(embedded_words.shape[0], sequence_length*self.embedding_size)  # batch_size, seq_length*embedding_size\n",
    "\n",
    "        z = self.hidden_layer(embedded_words)   # (batch_size, seq_length, hidden_size)\n",
    "        \n",
    "        ### ADD THE MISSING LINE HERE\n",
    "        \n",
    "        ########\n",
    "\n",
    "        output = self.output_layer(h)                      # (batch_size, num_classes)\n",
    "\n",
    "        # Notice we haven't applied a softmax activation to the output layer -- it's not required by Pytorch's loss function.\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06f22db-b953-4f98-920f-3f9a2b1e3707",
   "metadata": {},
   "source": [
    "Now the class is complete. \n",
    "\n",
    "TO-DO 2c: In the next cell, create a NN with the FFTextClassifier class we wrote. (unmarked)\n",
    "\n",
    "Hint: `ff_classifier_model = FFTextClassifier(...)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52d795a-adbc-4c48-aadf-f911520ac42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vectorizer.vocabulary_) + 1\n",
    "embedding_size = 10  # number of dimensions for embeddings\n",
    "hidden_size = 8 # number of hidden units\n",
    "\n",
    "###WRITE YOUR OWN CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a6c48d-f2b4-45e6-838e-d08c895f7e86",
   "metadata": {},
   "source": [
    "After desigining our network, we need to create a training function to calculate the loss for each input and perform backpropagation to optimise the network.\n",
    "During training, the weights of all the layers will be updated.\n",
    "\n",
    "Below, we build a training function to train the NN over a fixed number of epochs (an epoch is one iteration over the whole training dataset).\n",
    "The function also prints the performance of both training and development/validation set after each epoch.\n",
    "\n",
    "Here we use cross-entropy loss, which is the standard loss function for classification that we also used for logistic regression. The module `nn.CrossEntropyLoss()` operates directly on the output of our output layer, so we don't have to implement the softmax layer within the forward() method.\n",
    "\n",
    "Cross Entropy Loss: https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "\n",
    "The optimizer object implements a particular algorithm for updating the weights. Here, we will use the Adam optimizer, which is a variant of stochastic gradient descent method that tends to find a better solution in a smaller number of iterations than standard SGD.\n",
    "\n",
    "Optimization: https://pytorch.org/docs/stable/optim.html\n",
    "\n",
    "The cell below defines a training function for our classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c682c18-b30c-489f-9c22-fb5662fc7960",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "def train_nn(num_epochs, model, train_dataloader, dev_dataloader):\n",
    "    \n",
    "    learning_rate = 0.0005  # learning rate for the gradient descent optimizer, related to the step size\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()  # create loss function object\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # create the optimizer\n",
    "        \n",
    "    for e in range(num_epochs):\n",
    "        # Track performance on the training set as we are learning...\n",
    "        total_correct = 0\n",
    "        total_trained = 0\n",
    "        train_losses = []\n",
    "\n",
    "        model.train()  # Put the model in training mode.\n",
    "\n",
    "        for i, (batch_input_ids, batch_labels) in enumerate(train_dataloader):\n",
    "            # Iterate over each batch of data\n",
    "            # print(f'batch no. = {i}')\n",
    "\n",
    "            optimizer.zero_grad()  # Reset the optimizer\n",
    "\n",
    "            # Use the model to perform forward inference on the input data.\n",
    "            # This will run the forward() function.\n",
    "            output = model(batch_input_ids)\n",
    "\n",
    "            # Compute the loss for the current batch of data\n",
    "            batch_loss = loss_fn(output, batch_labels)\n",
    "\n",
    "            # Perform back propagation to compute the gradients with respect to each weight\n",
    "            batch_loss.backward()\n",
    "\n",
    "            # Update the weights using the compute gradients\n",
    "            optimizer.step()\n",
    "\n",
    "            # Record the loss from this sample to keep track of progress.\n",
    "            train_losses.append(batch_loss.item())\n",
    "\n",
    "            # Count correct labels so we can compute accuracy on the training set\n",
    "            predicted_labels = output.argmax(1)\n",
    "            total_correct += (predicted_labels == batch_labels).sum().item()\n",
    "            total_trained += batch_labels.size(0)\n",
    "\n",
    "        train_accuracy = total_correct/total_trained*100\n",
    "\n",
    "        print(\"Epoch: {}/{}\".format((e+1), num_epochs),\n",
    "              \"Training Loss: {:.4f}\".format(np.mean(train_losses)),\n",
    "              \"Training Accuracy: {:.4f}%\".format(train_accuracy))\n",
    "        \n",
    "        # Compute accuracy on dev set after this training epoch\n",
    "        \n",
    "        model.eval()  # Switch model to evaluation mode - turn off any random steps such as dropout\n",
    "        total_correct = 0\n",
    "        total_trained = 0\n",
    "        dev_losses = []\n",
    "\n",
    "        for dev_input_ids, dev_labels in dev_dataloader:\n",
    "            \n",
    "            # Count the number of correct predictions\n",
    "            predicted_labels = dev_output.argmax(1)\n",
    "            total_correct += (predicted_labels == dev_labels).sum().item()\n",
    "            total_trained += dev_labels.size(0)\n",
    "            \n",
    "        dev_accuracy = total_correct/total_trained*100\n",
    "        \n",
    "        print(\"Epoch: {}/{}\".format((e+1), num_epochs),\n",
    "              \"Validation Loss: {:.4f}\".format(np.mean(dev_losses)),\n",
    "              \"Validation Accuracy: {:.4f}%\".format(dev_accuracy))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e473f92-78d4-4a5d-94cc-749165e3896e",
   "metadata": {},
   "source": [
    "**TO-DO 2d:** Modify the training function above to return the training and development (or 'validation') losses at each epoch. Train the network for 15 epochs and plot the losses. Describe what the plot shows, and how you could use this information to improve the training process. **(8 marks)**\n",
    "\n",
    "EXPLAIN YOUR ANSWER HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac7c289-fae8-4045-b18c-8db7e678214e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###WRITE YOUR OWN CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d655f07b-c687-4d9b-a0b3-44f0d3ccbead",
   "metadata": {},
   "source": [
    "The code below obtains predictions from our neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4368a8-57fc-4cd3-b74e-aa385105c91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_nn(trained_model, test_loader):\n",
    "\n",
    "    trained_model.eval()  # switch off some randomisation used during training (dropout) to give consistent predictions\n",
    "\n",
    "    correct = 0  # count the number of correct classification labels\n",
    "\n",
    "    gold_labs = []  # gold labels to return\n",
    "    pred_labs = []  # predicted labels to return\n",
    "    \n",
    "    for inputs, labels in test_loader:\n",
    "        test_output = trained_model(inputs)  # run the forward() function on the inputs\n",
    "        predicted_labels = test_output.argmax(1)  # select the class labels with highest logits as our predictions\n",
    "\n",
    "        gold_labs.extend(labels.tolist())\n",
    "        pred_labs.extend(predicted_labels.tolist())\n",
    "    \n",
    "    return gold_labs, pred_labs\n",
    "\n",
    "gold_labs, pred_labs = predict_nn(trained_model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5620658-9e5a-4248-aa48-3e3868011aaf",
   "metadata": {},
   "source": [
    "Now, we can use pretrained word embeddings instead of learning them from scratch during training.\n",
    "Here, we will use the pretrained GloVe embeddings that we loaded before. The embedding matrix is used to initialise the embedding layer. The code below converts the GloVe embeddings into an embedding matrix suitable for PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872638e9-f86c-48e4-8745-9c667e250a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = torch.zeros((vocab_size, glove_wv.vector_size))\n",
    "for word in vocab:\n",
    "    word_idx = vocab[word]\n",
    "    if word in glove_wv:\n",
    "        embedding_matrix[word_idx, :] = torch.from_numpy(glove_wv[word])\n",
    "        \n",
    "print(embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850e7656-343f-449c-9329-411ad7fb22d5",
   "metadata": {},
   "source": [
    "The class below extends the FFTextClassifier class (it's incomplete for now -- you'll fix this in a minute!). This means that it inherits all of its functionality, but we overwrite the constructor (the `__init__` method). This way, we don't need to define the forward function again, as it will be the same as before.\n",
    "\n",
    "The embedding layer is now different as it loads pretrained embeddings from our matrix. The argument `freeze` determines whether the embeddings remain fixed to their pretrained values (if `freeze=True`) or are updated through backpropagation to fit them to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b93f97-57b9-4d3b-a36f-547d472e1f37",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class FFTextClassifierWithEmbeddings(FFTextClassifier):\n",
    "\n",
    "    def __init__(self, hidden_size, sequence_length, num_classes, embedding_matrix):\n",
    "        super(FFTextClassifier, self).__init__()\n",
    "\n",
    "        self.embedding_size = embedding_matrix.shape[1] \n",
    "\n",
    "        # Here we just need to construct the components of our network. We don't need to connect them together yet.\n",
    "        self.embedding_layer = nn.Embedding.from_pretrained(embedding_matrix, freeze=True) # embedding layer\n",
    "\n",
    "        ### COMPLETE THE ARGUMENTS TO SPECIFY THE DIMENSIONS OF THE LAYERS\n",
    "        self.hidden_layer = nn.Linear(       ) # Hidden layer\n",
    "        self.activation = nn.ReLU(      ) # Hidden layer activation\n",
    "        self.output_layer = nn.Linear(      ) # Fully connected layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eae91af-4e37-476d-ba7d-709924bb329c",
   "metadata": {},
   "source": [
    "**TO-DO 2e:** Complete the arguments in the `FFTextClassifierWithEmbeddings` constructor to set the dimensions of the neural network layers.  Repeat the experiment above using the FFTextClassifierWithEmbeddings with the GLoVe embeddings. Choose a suitable performance metric and compare the performance of the two neural text classifiers. Explain in one or two sentences the possible reason(s) for any performance differences you observe. **(3 marks)**\n",
    "\n",
    "WRITE YOUR ANSWER HERE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d4afd4-a92d-468d-a6ea-e693c10de36d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### WRITE YOUR OWN CODE BELOW\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3c20d5-12c8-4820-89e2-e53c11b77b42",
   "metadata": {},
   "source": [
    "# 3. Improving the Neural Text Classifier (max. 22 marks)\n",
    "\n",
    "This section allows you some more free reign to experiment with the neural text classifier. Below, we list several to-dos that you can solve in your own way. Please make sure to label your notebook cells clearly so that it is obvious which to-do each cell corresponds to.\n",
    "\n",
    "**TO-DO 3a:** Consider the neural text classifiers we have just implemented and the results you obtained in the last to-do. The classifiers have a number of limitations that we could improve. Describe three limitations and how you could improve them. For each improvement you propose, provide a brief explanation (up to 1 paragraph) of how it works. \n",
    "\n",
    "Hint: refer to the lectures for some ideas. **(9 marks)**\n",
    "\n",
    "WRITE YOUR ANSWER HERE:\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**TO-DO 3b:** Implement your improvements and compute the performance of your method. Make sure to comment your code to show where each new step is implemented. Use the validation set for any tuning you decide to do. Present your results clearly. **(13 marks)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99e0d0d-ac93-4f7b-8169-c81f6b151eed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analytics",
   "language": "python",
   "name": "data_analytics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
